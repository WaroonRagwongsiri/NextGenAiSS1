{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in ./miniconda3/envs/env/lib/python3.9/site-packages (0.19.1)\n",
      "Requirement already satisfied: numpy in ./miniconda3/envs/env/lib/python3.9/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: torch==2.4.1 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torchvision) (2.4.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (1.13.2)\n",
      "Requirement already satisfied: networkx in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.4.1->torchvision) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./miniconda3/envs/env/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchvision) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/envs/env/lib/python3.9/site-packages (from jinja2->torch==2.4.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/envs/env/lib/python3.9/site-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: pandas in ./miniconda3/envs/env/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./miniconda3/envs/env/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/envs/env/lib/python3.9/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/envs/env/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./miniconda3/envs/env/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/envs/env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/envs/env/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./miniconda3/envs/env/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./miniconda3/envs/env/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./miniconda3/envs/env/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./miniconda3/envs/env/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "!pip install pandas\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จำนวนข้อมูลทั้งหมด: 2888\n",
      "จำนวนข้อมูลสำหรับ training: 2310\n",
      "จำนวนข้อมูลสำหรับ validation: 578\n",
      "ขนาดของรูปภาพ: torch.Size([3, 256, 256])\n",
      "ตัวอย่าง sample_three_d (encoded): 1\n",
      "ตัวอย่าง sample_symmetric (encoded): 0\n",
      "ตัวอย่าง sample_food (encoded): 0\n",
      "ตัวอย่าง sample_person (encoded): 1\n",
      "ตัวอย่าง sample_nature (encoded): 0\n",
      "ตัวอย่าง sample_architectural (encoded): 0\n",
      "ตัวอย่าง sample_animal (encoded): 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define custom dataset\n",
    "class ImageLabelDataset(nn.Module):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Encode labels\n",
    "        self.three_d_encoder = LabelEncoder()\n",
    "        self.symmetric_encoder = LabelEncoder()\n",
    "        self.food_encoder = LabelEncoder()\n",
    "        self.person_encoder = LabelEncoder()\n",
    "        self.nature_encoder = LabelEncoder()\n",
    "        self.architectural_encoder = LabelEncoder()\n",
    "        self.animal_encoder = LabelEncoder()\n",
    "\n",
    "        self.data['three_d_encoded'] = self.three_d_encoder.fit_transform(self.data['3D'])\n",
    "        self.data['symmetric_encoded'] = self.symmetric_encoder.fit_transform(self.data['symmetric'])\n",
    "        self.data['food_encoded'] = self.food_encoder.fit_transform(self.data['food'])\n",
    "        self.data['person_encoded'] = self.person_encoder.fit_transform(self.data['person'])\n",
    "        self.data['nature_encoded'] = self.nature_encoder.fit_transform(self.data['nature'])\n",
    "        self.data['architectural_encoded'] = self.architectural_encoder.fit_transform(self.data['architectural'])\n",
    "        self.data['animal_encoded'] = self.animal_encoder.fit_transform(self.data['animal'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.data.iloc[idx]['image_path'])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        three_d = self.data.iloc[idx]['three_d_encoded']\n",
    "        symmetric = self.data.iloc[idx]['symmetric_encoded']\n",
    "        food = self.data.iloc[idx]['food_encoded']\n",
    "        person = self.data.iloc[idx]['person_encoded']\n",
    "        nature = self.data.iloc[idx]['nature_encoded']\n",
    "        architectural = self.data.iloc[idx]['architectural_encoded']\n",
    "        animal = self.data.iloc[idx]['animal_encoded']\n",
    "\n",
    "        return image, three_d, symmetric, food, person ,nature ,architectural ,animal\n",
    "\n",
    "#transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# dataset\n",
    "dataset = ImageLabelDataset(csv_file='Day1/dataset1/NextGenClassification/train_classification.csv', img_dir='Day1/dataset1/NextGenClassification', transform=transform)\n",
    "\n",
    "# train and validate\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"จำนวนข้อมูลทั้งหมด:\", len(dataset))\n",
    "print(\"จำนวนข้อมูลสำหรับ training:\", len(train_data))\n",
    "print(\"จำนวนข้อมูลสำหรับ validation:\", len(val_data))\n",
    "\n",
    "sample_image,sample_three_d, sample_symmetric, sample_food, sample_person ,sample_nature ,sample_architectural ,sample_animal = dataset[0]\n",
    "print(\"ขนาดของรูปภาพ:\", sample_image.shape)\n",
    "print(\"ตัวอย่าง sample_three_d (encoded):\", sample_three_d)\n",
    "print(\"ตัวอย่าง sample_symmetric (encoded):\", sample_symmetric)\n",
    "print(\"ตัวอย่าง sample_food (encoded):\", sample_food)\n",
    "print(\"ตัวอย่าง sample_person (encoded):\", sample_person)\n",
    "print(\"ตัวอย่าง sample_nature (encoded):\", sample_nature)\n",
    "print(\"ตัวอย่าง sample_architectural (encoded):\", sample_architectural)\n",
    "print(\"ตัวอย่าง sample_animal (encoded):\", sample_animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageLabelModel(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): ReLU(inplace=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (three_d_classifier): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (symmetric_classifier): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (food_classifier): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (person_classifier): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (nature_classifier): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (architectural_classifier): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (animal_classifier): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ImageLabelModel(nn.Module):\n",
    "    def __init__(self, num_three_d, num_symmetric, num_food, num_person ,num_nature ,num_architectural ,num_animal):\n",
    "        super(ImageLabelModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        self.three_d_classifier = nn.Linear(512, num_three_d)\n",
    "        self.symmetric_classifier = nn.Linear(512, num_symmetric)\n",
    "        self.food_classifier = nn.Linear(512, num_food)\n",
    "        self.person_classifier = nn.Linear(512, num_person)\n",
    "        self.nature_classifier = nn.Linear(512, num_nature)\n",
    "        self.architectural_classifier = nn.Linear(512, num_architectural)\n",
    "        self.animal_classifier = nn.Linear(512, num_animal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        three_d = self.three_d_classifier(x)\n",
    "        symmetric = self.symmetric_classifier(x)\n",
    "        food = self.food_classifier(x)\n",
    "        person = self.person_classifier(x)\n",
    "        nature = self.nature_classifier(x)\n",
    "        architectural = self.architectural_classifier(x)\n",
    "        animal = self.animal_classifier(x)\n",
    "        return three_d, symmetric, food, person ,nature ,architectural ,animal\n",
    "\n",
    "# Create the model instance\n",
    "model = ImageLabelModel(\n",
    "    num_three_d=len(dataset.three_d_encoder.classes_),\n",
    "    num_symmetric=len(dataset.symmetric_encoder.classes_),\n",
    "    num_food=len(dataset.food_encoder.classes_),\n",
    "    num_person=len(dataset.person_encoder.classes_),\n",
    "    num_nature=len(dataset.nature_encoder.classes_),\n",
    "    num_architectural=len(dataset.architectural_encoder.classes_),\n",
    "    num_animal=len(dataset.animal_encoder.classes_),\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camp-ai-28/miniconda3/envs/env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define loss function and optimizer\n",
    "def weighted_loss(three_d_out, symmetric_out, food_out, person_out ,nature_out ,architectural_out ,animal_out, three_d, symmetric, food, person ,nature ,architectural ,animal):\n",
    "    three_d_loss = criterion(three_d_out, three_d)\n",
    "    symmetric_loss = criterion(symmetric_out, symmetric)\n",
    "    food_loss = criterion(food_out, food)\n",
    "    person_loss = criterion(person_out, person)\n",
    "    nature_loss = criterion(nature_out, nature)\n",
    "    architectural_loss = criterion(architectural_out, architectural)\n",
    "    animal_loss = criterion(animal_out, animal)\n",
    "    return 0.15 * three_d_loss + 0.14 * symmetric_loss + 0.14 * food_loss + 0.14 * person_loss + 0.15 * nature_loss + 0.14 * architectural_loss + 0.14 * animal_loss\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Training loss: 0.4138\n",
      "Validation loss: 0.3634\n",
      "3D accuracy: 79.24%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 55.54%\n",
      "Nature accuracy: 60.73%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 2/100\n",
      "Training loss: 0.3720\n",
      "Validation loss: 0.3268\n",
      "3D accuracy: 79.24%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 64.36%\n",
      "Nature accuracy: 70.24%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 3/100\n",
      "Training loss: 0.3222\n",
      "Validation loss: 0.3113\n",
      "3D accuracy: 79.24%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 67.13%\n",
      "Nature accuracy: 75.78%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 4/100\n",
      "Training loss: 0.2892\n",
      "Validation loss: 0.2755\n",
      "3D accuracy: 79.24%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 74.22%\n",
      "Nature accuracy: 85.81%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 5/100\n",
      "Training loss: 0.2688\n",
      "Validation loss: 0.2429\n",
      "3D accuracy: 79.41%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 78.55%\n",
      "Nature accuracy: 89.45%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 6/100\n",
      "Training loss: 0.2549\n",
      "Validation loss: 0.2453\n",
      "3D accuracy: 76.30%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 79.58%\n",
      "Nature accuracy: 91.00%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 7/100\n",
      "Training loss: 0.2444\n",
      "Validation loss: 0.2416\n",
      "3D accuracy: 77.85%\n",
      "Symmetric accuracy: 96.02%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 82.35%\n",
      "Nature accuracy: 89.27%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 8/100\n",
      "Training loss: 0.2427\n",
      "Validation loss: 0.2259\n",
      "3D accuracy: 79.24%\n",
      "Symmetric accuracy: 95.85%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 85.64%\n",
      "Nature accuracy: 91.87%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 9/100\n",
      "Training loss: 0.2262\n",
      "Validation loss: 0.2302\n",
      "3D accuracy: 78.20%\n",
      "Symmetric accuracy: 96.02%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 84.26%\n",
      "Nature accuracy: 89.62%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 10/100\n",
      "Training loss: 0.2251\n",
      "Validation loss: 0.2131\n",
      "3D accuracy: 79.41%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 88.24%\n",
      "Nature accuracy: 93.60%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 11/100\n",
      "Training loss: 0.2203\n",
      "Validation loss: 0.2166\n",
      "3D accuracy: 79.07%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 86.33%\n",
      "Nature accuracy: 92.56%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 12/100\n",
      "Training loss: 0.2142\n",
      "Validation loss: 0.2250\n",
      "3D accuracy: 79.41%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 83.56%\n",
      "Nature accuracy: 92.04%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 13/100\n",
      "Training loss: 0.2186\n",
      "Validation loss: 0.2194\n",
      "3D accuracy: 79.24%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 85.29%\n",
      "Nature accuracy: 91.52%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 14/100\n",
      "Training loss: 0.2081\n",
      "Validation loss: 0.2259\n",
      "3D accuracy: 79.24%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 87.37%\n",
      "Nature accuracy: 92.56%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 15/100\n",
      "Training loss: 0.2014\n",
      "Validation loss: 0.2075\n",
      "3D accuracy: 79.07%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 89.27%\n",
      "Nature accuracy: 93.94%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 16/100\n",
      "Training loss: 0.1974\n",
      "Validation loss: 0.2304\n",
      "3D accuracy: 80.28%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 86.33%\n",
      "Nature accuracy: 91.00%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 17/100\n",
      "Training loss: 0.1999\n",
      "Validation loss: 0.2048\n",
      "3D accuracy: 80.97%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 89.27%\n",
      "Nature accuracy: 93.60%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 18/100\n",
      "Training loss: 0.1962\n",
      "Validation loss: 0.2069\n",
      "3D accuracy: 81.66%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.06%\n",
      "Person accuracy: 90.31%\n",
      "Nature accuracy: 93.08%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 19/100\n",
      "Training loss: 0.1884\n",
      "Validation loss: 0.2103\n",
      "3D accuracy: 82.01%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 89.27%\n",
      "Nature accuracy: 93.08%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 20/100\n",
      "Training loss: 0.1834\n",
      "Validation loss: 0.2113\n",
      "3D accuracy: 82.70%\n",
      "Symmetric accuracy: 95.33%\n",
      "Food accuracy: 97.06%\n",
      "Person accuracy: 89.62%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 21/100\n",
      "Training loss: 0.1809\n",
      "Validation loss: 0.1998\n",
      "3D accuracy: 81.83%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.06%\n",
      "Person accuracy: 90.66%\n",
      "Nature accuracy: 94.81%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 22/100\n",
      "Training loss: 0.1814\n",
      "Validation loss: 0.2007\n",
      "3D accuracy: 82.70%\n",
      "Symmetric accuracy: 95.85%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.94%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 23/100\n",
      "Training loss: 0.1725\n",
      "Validation loss: 0.2157\n",
      "3D accuracy: 80.45%\n",
      "Symmetric accuracy: 95.85%\n",
      "Food accuracy: 97.40%\n",
      "Person accuracy: 89.62%\n",
      "Nature accuracy: 91.35%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 24/100\n",
      "Training loss: 0.1828\n",
      "Validation loss: 0.1965\n",
      "3D accuracy: 82.35%\n",
      "Symmetric accuracy: 96.02%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 89.97%\n",
      "Nature accuracy: 94.12%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 25/100\n",
      "Training loss: 0.1676\n",
      "Validation loss: 0.2082\n",
      "3D accuracy: 82.87%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 88.41%\n",
      "Nature accuracy: 94.29%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 26/100\n",
      "Training loss: 0.1626\n",
      "Validation loss: 0.2075\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.06%\n",
      "Person accuracy: 89.97%\n",
      "Nature accuracy: 94.81%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 27/100\n",
      "Training loss: 0.1564\n",
      "Validation loss: 0.2071\n",
      "3D accuracy: 81.83%\n",
      "Symmetric accuracy: 95.85%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 91.70%\n",
      "Nature accuracy: 93.43%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 28/100\n",
      "Training loss: 0.1502\n",
      "Validation loss: 0.2017\n",
      "3D accuracy: 82.35%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.40%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.94%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 29/100\n",
      "Training loss: 0.1466\n",
      "Validation loss: 0.2066\n",
      "3D accuracy: 82.87%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.23%\n",
      "Person accuracy: 90.83%\n",
      "Nature accuracy: 93.43%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 30/100\n",
      "Training loss: 0.1389\n",
      "Validation loss: 0.2146\n",
      "3D accuracy: 82.01%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 90.83%\n",
      "Nature accuracy: 93.60%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 31/100\n",
      "Training loss: 0.1199\n",
      "Validation loss: 0.1975\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.40%\n",
      "Person accuracy: 91.52%\n",
      "Nature accuracy: 94.12%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 32/100\n",
      "Training loss: 0.1125\n",
      "Validation loss: 0.1991\n",
      "3D accuracy: 82.01%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.40%\n",
      "Person accuracy: 91.52%\n",
      "Nature accuracy: 93.60%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 33/100\n",
      "Training loss: 0.1088\n",
      "Validation loss: 0.2084\n",
      "3D accuracy: 83.39%\n",
      "Symmetric accuracy: 95.67%\n",
      "Food accuracy: 97.40%\n",
      "Person accuracy: 91.87%\n",
      "Nature accuracy: 93.60%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 34/100\n",
      "Training loss: 0.1077\n",
      "Validation loss: 0.2055\n",
      "3D accuracy: 82.35%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.18%\n",
      "Nature accuracy: 93.60%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 35/100\n",
      "Training loss: 0.1055\n",
      "Validation loss: 0.2106\n",
      "3D accuracy: 82.35%\n",
      "Symmetric accuracy: 95.16%\n",
      "Food accuracy: 97.40%\n",
      "Person accuracy: 91.35%\n",
      "Nature accuracy: 93.43%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 36/100\n",
      "Training loss: 0.1018\n",
      "Validation loss: 0.2113\n",
      "3D accuracy: 82.35%\n",
      "Symmetric accuracy: 95.33%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 90.48%\n",
      "Nature accuracy: 93.60%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 37/100\n",
      "Training loss: 0.1011\n",
      "Validation loss: 0.2115\n",
      "3D accuracy: 82.35%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.40%\n",
      "Person accuracy: 90.83%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 38/100\n",
      "Training loss: 0.0993\n",
      "Validation loss: 0.2135\n",
      "3D accuracy: 82.01%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.40%\n",
      "Person accuracy: 90.83%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 39/100\n",
      "Training loss: 0.1002\n",
      "Validation loss: 0.2124\n",
      "3D accuracy: 82.53%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.18%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 40/100\n",
      "Training loss: 0.0991\n",
      "Validation loss: 0.2138\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.40%\n",
      "Person accuracy: 91.18%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 41/100\n",
      "Training loss: 0.0994\n",
      "Validation loss: 0.2150\n",
      "3D accuracy: 82.01%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.40%\n",
      "Person accuracy: 90.66%\n",
      "Nature accuracy: 93.43%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 42/100\n",
      "Training loss: 0.0976\n",
      "Validation loss: 0.2138\n",
      "3D accuracy: 82.53%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 43/100\n",
      "Training loss: 0.0979\n",
      "Validation loss: 0.2141\n",
      "3D accuracy: 82.53%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 44/100\n",
      "Training loss: 0.0974\n",
      "Validation loss: 0.2144\n",
      "3D accuracy: 82.53%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 45/100\n",
      "Training loss: 0.0995\n",
      "Validation loss: 0.2145\n",
      "3D accuracy: 82.53%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 46/100\n",
      "Training loss: 0.0991\n",
      "Validation loss: 0.2146\n",
      "3D accuracy: 82.53%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 47/100\n",
      "Training loss: 0.0971\n",
      "Validation loss: 0.2148\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 48/100\n",
      "Training loss: 0.0961\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 49/100\n",
      "Training loss: 0.0975\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 50/100\n",
      "Training loss: 0.0958\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 51/100\n",
      "Training loss: 0.0969\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 52/100\n",
      "Training loss: 0.0970\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 53/100\n",
      "Training loss: 0.0975\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 54/100\n",
      "Training loss: 0.0962\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 55/100\n",
      "Training loss: 0.0976\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 56/100\n",
      "Training loss: 0.0986\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 57/100\n",
      "Training loss: 0.0981\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 58/100\n",
      "Training loss: 0.1005\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 59/100\n",
      "Training loss: 0.0972\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 60/100\n",
      "Training loss: 0.0970\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 61/100\n",
      "Training loss: 0.0977\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 62/100\n",
      "Training loss: 0.0961\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 63/100\n",
      "Training loss: 0.0978\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 64/100\n",
      "Training loss: 0.0991\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 65/100\n",
      "Training loss: 0.0981\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 66/100\n",
      "Training loss: 0.0991\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 67/100\n",
      "Training loss: 0.0966\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 68/100\n",
      "Training loss: 0.0980\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 69/100\n",
      "Training loss: 0.0965\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 70/100\n",
      "Training loss: 0.0973\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 71/100\n",
      "Training loss: 0.0981\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 72/100\n",
      "Training loss: 0.0969\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 73/100\n",
      "Training loss: 0.0984\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 74/100\n",
      "Training loss: 0.0981\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 75/100\n",
      "Training loss: 0.0996\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 76/100\n",
      "Training loss: 0.0985\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 77/100\n",
      "Training loss: 0.0974\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 78/100\n",
      "Training loss: 0.0978\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 79/100\n",
      "Training loss: 0.0974\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 80/100\n",
      "Training loss: 0.0993\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 81/100\n",
      "Training loss: 0.0991\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 82/100\n",
      "Training loss: 0.0978\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 83/100\n",
      "Training loss: 0.0978\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 84/100\n",
      "Training loss: 0.0956\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 85/100\n",
      "Training loss: 0.0980\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 86/100\n",
      "Training loss: 0.0995\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 87/100\n",
      "Training loss: 0.0979\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 88/100\n",
      "Training loss: 0.0983\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 89/100\n",
      "Training loss: 0.0966\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 90/100\n",
      "Training loss: 0.0981\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 91/100\n",
      "Training loss: 0.0987\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 92/100\n",
      "Training loss: 0.0977\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 93/100\n",
      "Training loss: 0.0989\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 94/100\n",
      "Training loss: 0.0985\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 95/100\n",
      "Training loss: 0.0982\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 96/100\n",
      "Training loss: 0.0977\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 97/100\n",
      "Training loss: 0.0971\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 98/100\n",
      "Training loss: 0.0965\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 99/100\n",
      "Training loss: 0.0990\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Epoch 100/100\n",
      "Training loss: 0.0985\n",
      "Validation loss: 0.2149\n",
      "3D accuracy: 82.18%\n",
      "Symmetric accuracy: 95.50%\n",
      "Food accuracy: 97.58%\n",
      "Person accuracy: 91.00%\n",
      "Nature accuracy: 93.77%\n",
      "Architectural accuracy: 98.96%\n",
      "Animal accuracy: 92.21%\n",
      "------------------------------------------------------------\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, three_d, symmetric, food, person ,nature ,architectural ,animal in train_loader:\n",
    "        images, three_d, symmetric, food, person ,nature ,architectural ,animal = images.to(device), three_d.to(device), symmetric.to(device), food.to(device), person.to(device) ,nature.to(device) ,architectural.to(device) ,animal.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        three_d_out, symmetric_out, food_out, person_out ,nature_out ,architectural_out ,animal_out = model(images)\n",
    "        loss = weighted_loss(three_d_out, symmetric_out, food_out, person_out ,nature_out ,architectural_out ,animal_out, three_d, symmetric, food, person ,nature ,architectural ,animal)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_three_d, correct_symmetric, correct_food, correct_person ,correct_nature ,correct_architectural ,correct_animal = 0, 0, 0, 0, 0, 0, 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, three_d, symmetric, food, person ,nature ,architectural ,animal in val_loader:\n",
    "            images, three_d, symmetric, food, person ,nature ,architectural ,animal = images.to(device), three_d.to(device), symmetric.to(device), food.to(device), person.to(device) ,nature.to(device) ,architectural.to(device) ,animal.to(device)\n",
    "\n",
    "            three_d_out, symmetric_out, food_out, person_out ,nature_out ,architectural_out ,animal_out = model(images)\n",
    "            loss = weighted_loss(three_d_out, symmetric_out, food_out, person_out ,nature_out ,architectural_out ,animal_out, three_d, symmetric, food, person ,nature ,architectural ,animal)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted_three_d = torch.max(three_d_out, 1)\n",
    "            _, predicted_symmetric = torch.max(symmetric_out, 1)\n",
    "            _, predicted_food = torch.max(food_out, 1)\n",
    "            _, predicted_person = torch.max(person_out, 1)\n",
    "            _, predicted_nature = torch.max(nature_out, 1)\n",
    "            _, predicted_architectural = torch.max(architectural_out, 1)\n",
    "            _, predicted_animal = torch.max(animal_out, 1)\n",
    "\n",
    "            total += three_d.size(0)\n",
    "            correct_three_d += (predicted_three_d == three_d).sum().item()\n",
    "            correct_symmetric += (predicted_symmetric == symmetric).sum().item()\n",
    "            correct_food += (predicted_food == food).sum().item()\n",
    "            correct_person += (predicted_person == person).sum().item()\n",
    "            correct_nature += (predicted_nature == nature).sum().item()\n",
    "            correct_architectural += (predicted_architectural == architectural).sum().item()\n",
    "            correct_animal += (predicted_animal == animal).sum().item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Training loss: {running_loss/len(train_loader):.4f}')\n",
    "    print(f'Validation loss: {val_loss/len(val_loader):.4f}')\n",
    "    print(f'3D accuracy: {100 * correct_three_d / total:.2f}%')\n",
    "    print(f'Symmetric accuracy: {100 * correct_symmetric / total:.2f}%')\n",
    "    print(f'Food accuracy: {100 * correct_food / total:.2f}%')\n",
    "    print(f'Person accuracy: {100 * correct_person / total:.2f}%')\n",
    "    print(f'Nature accuracy: {100 * correct_nature / total:.2f}%')\n",
    "    print(f'Architectural accuracy: {100 * correct_architectural / total:.2f}%')\n",
    "    print(f'Animal accuracy: {100 * correct_animal / total:.2f}%')\n",
    "    print('-' * 60)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save the model\n",
    "torch.save(model.state_dict(), 'Day1/weight/image_label_model.pth')\n",
    "print(\"Model saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3603222/3769205738.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('Day1/weight/image_label_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day1/submission/waroon_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load and evaluate on test set\n",
    "model = ImageLabelModel(\n",
    "    num_three_d=len(dataset.three_d_encoder.classes_),\n",
    "    num_symmetric=len(dataset.symmetric_encoder.classes_),\n",
    "    num_food=len(dataset.food_encoder.classes_),\n",
    "    num_person=len(dataset.person_encoder.classes_),\n",
    "    num_nature=len(dataset.nature_encoder.classes_),\n",
    "    num_architectural=len(dataset.architectural_encoder.classes_),\n",
    "    num_animal=len(dataset.animal_encoder.classes_),\n",
    ")\n",
    "model.load_state_dict(torch.load('Day1/weight/image_label_model.pth'))\n",
    "model = model.to(device)\n",
    "\n",
    "def analyze_errors(model, data_loader):\n",
    "    model.eval()\n",
    "    three_d_errors, symmetric_errors, food_errors, person_errors ,nature_errors ,architectural_errors ,animal_errors = 0, 0, 0, 0, 0, 0, 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, three_d, symmetric, food, person ,nature ,architectural ,animal in data_loader:\n",
    "            images, three_d, symmetric, food, person ,nature ,architectural ,animal = images.to(device), three_d.to(device), symmetric.to(device), food.to(device), person.to(device) ,nature.to(device) ,architectural.to(device) ,animal.to(device)\n",
    "            three_d_out, symmetric_out, food_out, person_out ,nature_out ,architectural_out ,animal_out = model(images)\n",
    "\n",
    "            _, predicted_three_d = torch.max(three_d_out, 1)\n",
    "            _, predicted_symmetric = torch.max(symmetric_out, 1)\n",
    "            _, predicted_food = torch.max(food_out, 1)\n",
    "            _, predicted_person = torch.max(person_out, 1)\n",
    "            _, predicted_nature = torch.max(nature_out, 1)\n",
    "            _, predicted_architectural = torch.max(architectural_out, 1)\n",
    "            _, predicted_animal = torch.max(animal_out, 1)\n",
    "\n",
    "            three_d_errors += (predicted_three_d != three_d).sum().item()\n",
    "            symmetric_errors += (predicted_symmetric != symmetric).sum().item()\n",
    "            food_errors += (predicted_food != food).sum().item()\n",
    "            person_errors += (predicted_person != person).sum().item()\n",
    "            nature_errors += (predicted_nature != nature).sum().item()\n",
    "            architectural_errors += (predicted_architectural != architectural).sum().item()\n",
    "            animal_errors += (predicted_animal != animal).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "    print(f\"3D Error Rate: {three_d_errors/total:.2%}\")\n",
    "    print(f\"Symmetric Error Rate: {symmetric_errors/total:.2%}\")\n",
    "    print(f\"Food Error Rate: {food_errors/total:.2%}\")\n",
    "    print(f\"Person Error Rate: {person_errors/total:.2%}\")\n",
    "    print(f\"nature Error Rate: {nature_errors/total:.2%}\")\n",
    "    print(f\"architectural Error Rate: {architectural_errors/total:.2%}\")\n",
    "    print(f\"animal Error Rate: {animal_errors/total:.2%}\")\n",
    "\n",
    "test_dataset = ImageLabelDataset(csv_file='Day1/dataset1/NextGenClassification/sample_submission.csv', img_dir='Day1/dataset1/NextGenClassification', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, _, _, _, _, _, _, _) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        three_d_out, symmetric_out, food_out, person_out ,nature_out ,architectural_out ,animal_out = model(images)\n",
    "\n",
    "        _, predicted_three_d = torch.max(three_d_out, 1)\n",
    "        _, predicted_symmetric = torch.max(symmetric_out, 1)\n",
    "        _, predicted_food = torch.max(food_out, 1)\n",
    "        _, predicted_person = torch.max(person_out, 1)\n",
    "        _, predicted_nature = torch.max(nature_out, 1)\n",
    "        _, predicted_architectural = torch.max(architectural_out, 1)\n",
    "        _, predicted_animal = torch.max(animal_out, 1)\n",
    "\n",
    "        for j, (three_d, symmetric, food, person ,nature ,architectural ,animal) in enumerate(zip(predicted_three_d, predicted_symmetric, predicted_food, predicted_person, predicted_nature, predicted_architectural, predicted_animal)):\n",
    "            index = i * test_loader.batch_size + j\n",
    "            predictions.append({\n",
    "                'image_path': test_dataset.data.iloc[index]['image_path'],\n",
    "                '3D': dataset.three_d_encoder.inverse_transform([three_d.item()])[0],\n",
    "                'symmetric': dataset.symmetric_encoder.inverse_transform([symmetric.item()])[0],\n",
    "                'food': dataset.food_encoder.inverse_transform([food.item()])[0],\n",
    "                'person': dataset.person_encoder.inverse_transform([person.item()])[0],\n",
    "                'nature': dataset.nature_encoder.inverse_transform([nature.item()])[0],\n",
    "                'architectural': dataset.architectural_encoder.inverse_transform([architectural.item()])[0],\n",
    "                'animal': dataset.animal_encoder.inverse_transform([animal.item()])[0],\n",
    "            })\n",
    "\n",
    "original_df = pd.read_csv('Day1/dataset1/NextGenClassification/sample_submission.csv')\n",
    "\n",
    "results_df = pd.DataFrame(predictions)\n",
    "\n",
    "original_df['3D'] = results_df['3D']\n",
    "original_df['symmetric'] = results_df['symmetric']\n",
    "original_df['food'] = results_df['food']\n",
    "original_df['person'] = results_df['person']\n",
    "original_df['nature'] = results_df['nature']\n",
    "original_df['architectural'] = results_df['architectural']\n",
    "original_df['animal'] = results_df['animal']\n",
    "\n",
    "original_df.to_csv(\"Day1/submission/waroon_submission.csv\",index=False)\n",
    "print(\"Day1/submission/waroon_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Error Rate: 17.82%\n",
      "Symmetric Error Rate: 4.50%\n",
      "Food Error Rate: 2.42%\n",
      "Person Error Rate: 9.00%\n",
      "nature Error Rate: 6.23%\n",
      "architectural Error Rate: 1.04%\n",
      "animal Error Rate: 7.79%\n"
     ]
    }
   ],
   "source": [
    "analyze_errors(model,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3603222/3537070496.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('Day1/weight/image_label_model.pth'))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m         f_beta_score \u001b[38;5;241m=\u001b[39m (three_d_f_beta_score \u001b[38;5;241m+\u001b[39m symmetric_f_beta_score \u001b[38;5;241m+\u001b[39m food_f_beta_score \u001b[38;5;241m+\u001b[39m person_f_beta_score \u001b[38;5;241m+\u001b[39m nature_f_beta_score \u001b[38;5;241m+\u001b[39m architectural_f_beta_score \u001b[38;5;241m+\u001b[39m animal_f_beta_score ) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mprint\u001b[39m(f_beta_score)\n\u001b[0;32m---> 91\u001b[0m \u001b[43mfbeta_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[187], line 80\u001b[0m, in \u001b[0;36mfbeta_submission\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     77\u001b[0m     y_predicted_animal\u001b[38;5;241m.\u001b[39mappend(predicted_animal)\n\u001b[1;32m     78\u001b[0m     y_true_animal\u001b[38;5;241m.\u001b[39mappend(animal)\n\u001b[0;32m---> 80\u001b[0m three_d_f_beta_score \u001b[38;5;241m=\u001b[39m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_three_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_predicted_three_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m symmetric_f_beta_score \u001b[38;5;241m=\u001b[39m fbeta_score(y_true_symmetric,y_predicted_symmetric,average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m,beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     82\u001b[0m food_f_beta_score \u001b[38;5;241m=\u001b[39m fbeta_score(y_true_food,y_predicted_food,average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m,beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1485\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1306\u001b[0m     {\n\u001b[1;32m   1307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1335\u001b[0m ):\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;124;03m    0.12...\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1789\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1627\u001b[0m \n\u001b[1;32m   1628\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1789\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1561\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1561\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:123\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    126\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n",
      "\u001b[0;31mValueError\u001b[0m: unknown is not supported"
     ]
    }
   ],
   "source": [
    "# # Cell test\n",
    "# y_predicted_three_d = []\n",
    "# y_true_three_d = []\n",
    "\n",
    "# y_predicted_symmetric = []\n",
    "# y_true_symmetric = []\n",
    "\n",
    "# y_predicted_food = []\n",
    "# y_true_food = []\n",
    "\n",
    "# y_predicted_person = []\n",
    "# y_true_person = []\n",
    "\n",
    "# y_predicted_nature = []\n",
    "# y_true_nature = []\n",
    "\n",
    "# y_predicted_architectural = []\n",
    "# y_true_architectural = []\n",
    "\n",
    "# y_predicted_animal = []\n",
    "# y_true_animal = []\n",
    "\n",
    "\n",
    "# model = ImageLabelModel(\n",
    "#     num_three_d=len(dataset.three_d_encoder.classes_),\n",
    "#     num_symmetric=len(dataset.symmetric_encoder.classes_),\n",
    "#     num_food=len(dataset.food_encoder.classes_),\n",
    "#     num_person=len(dataset.person_encoder.classes_),\n",
    "#     num_nature=len(dataset.nature_encoder.classes_),\n",
    "#     num_architectural=len(dataset.architectural_encoder.classes_),\n",
    "#     num_animal=len(dataset.animal_encoder.classes_),\n",
    "# )\n",
    "# model.load_state_dict(torch.load('Day1/weight/image_label_model.pth'))\n",
    "# model = model.to(device)\n",
    "\n",
    "# def fbeta_submission(model,data_loader):\n",
    "#     with torch.no_grad():\n",
    "#         for images, three_d, symmetric, food, person ,nature ,architectural ,animal in data_loader:\n",
    "#             images, three_d, symmetric, food, person ,nature ,architectural ,animal = images.to(device), three_d.to(device), symmetric.to(device), food.to(device), person.to(device) ,nature.to(device) ,architectural.to(device) ,animal.to(device)\n",
    "#             three_d_out, symmetric_out, food_out, person_out ,nature_out ,architectural_out ,animal_out = model(images)\n",
    "\n",
    "#             _, predicted_three_d = torch.max(three_d_out, 1)\n",
    "#             _, predicted_symmetric = torch.max(symmetric_out, 1)\n",
    "#             _, predicted_food = torch.max(food_out, 1)\n",
    "#             _, predicted_person = torch.max(person_out, 1)\n",
    "#             _, predicted_nature = torch.max(nature_out, 1)\n",
    "#             _, predicted_architectural = torch.max(architectural_out, 1)\n",
    "#             _, predicted_animal = torch.max(animal_out, 1)\n",
    "\n",
    "#             predicted_three_d = predicted_three_d.cpu().numpy()\n",
    "#             three_d = three_d.cpu().numpy()\n",
    "#             predicted_symmetric = predicted_symmetric.cpu().numpy()\n",
    "#             symmetric = symmetric.cpu().numpy()\n",
    "#             predicted_food = predicted_food.cpu().numpy()\n",
    "#             food = food.cpu().numpy()\n",
    "#             predicted_person = predicted_person.cpu().numpy()\n",
    "#             person = person.cpu().numpy()\n",
    "#             predicted_nature = predicted_nature.cpu().numpy()\n",
    "#             nature = nature.cpu().numpy()\n",
    "#             predicted_architectural = predicted_architectural.cpu().numpy()\n",
    "#             architectural = architectural.cpu().numpy()\n",
    "#             predicted_animal = predicted_animal.cpu().numpy()\n",
    "#             animal = animal.cpu().numpy()\n",
    "\n",
    "#             y_predicted_three_d.append(predicted_three_d)\n",
    "#             y_true_three_d.append(three_d)\n",
    "#             y_predicted_symmetric.append(predicted_symmetric)\n",
    "#             y_true_symmetric.append(symmetric)\n",
    "#             y_predicted_food.append(predicted_food)\n",
    "#             y_true_food.append(food)\n",
    "#             y_predicted_person.append(predicted_person)\n",
    "#             y_true_person.append(person)\n",
    "#             y_predicted_nature.append(predicted_nature)\n",
    "#             y_true_nature.append(nature)\n",
    "#             y_predicted_architectural.append(predicted_architectural)\n",
    "#             y_true_architectural.append(architectural)\n",
    "#             y_predicted_animal.append(predicted_animal)\n",
    "#             y_true_animal.append(animal)\n",
    "\n",
    "#         three_d_f_beta_score = fbeta_score(y_true_three_d, y_predicted_three_d, average='macro', beta=0.5)\n",
    "#         symmetric_f_beta_score = fbeta_score(y_true_symmetric,y_predicted_symmetric,average='macro',beta=0.5)\n",
    "#         food_f_beta_score = fbeta_score(y_true_food,y_predicted_food,average='macro',beta=0.5)\n",
    "#         person_f_beta_score = fbeta_score(y_true_person,y_predicted_person,average='macro',beta=0.5)\n",
    "#         nature_f_beta_score = fbeta_score(y_true_nature,y_predicted_nature,average='macro',beta=0.5)\n",
    "#         architectural_f_beta_score = fbeta_score(y_true_architectural,y_predicted_architectural,average='macro',beta=0.5)\n",
    "#         animal_f_beta_score = fbeta_score(y_true_animal,y_predicted_animal,average='macro',beta=0.5)\n",
    "\n",
    "#         f_beta_score = (three_d_f_beta_score + symmetric_f_beta_score + food_f_beta_score + person_f_beta_score + nature_f_beta_score + architectural_f_beta_score + animal_f_beta_score ) / 7\n",
    "#         print(f_beta_score)\n",
    "\n",
    "# fbeta_submission(model,val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
